{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request, jsonify, json, redirect\n",
    "from flask import render_template\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muddassarsharif/Desktop/2. mltrons code/production/env_experiment_lab/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.kde module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from WebApplication.model import *\n",
    "from WebApplication.utils import *\n",
    "from WebApplication.individual_explanation import *\n",
    "from WebApplication.global_explanations import *\n",
    "from WebApplication.queries import *\n",
    "from WebApplication.d3_functions import *\n",
    "from WebApplication.preprocessing import create_summary_file\n",
    "from WebApplication.distance_function import generate_projection_files, reduce_raw_data\n",
    "from WebApplication.projection import show_projection2, full_projection\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WebApplication.utils.dataset"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============= Initialize model =========== #\n",
    "\n",
    "# --- Setting random seed --- \n",
    "np.random.seed(150)\n",
    "\n",
    "# --- Resets all stored files ---\n",
    "reset = False\n",
    "\n",
    "# --- Dataset Selection ---\n",
    "admissions_dataset = dataset(\"admissions\", [6]) # (Conversion : Good > 0.7 )\n",
    "diabetes_dataset = dataset(\"diabetes\", [])\n",
    "fico_dataset = dataset(\"fico\", [0])\n",
    "heart_dataset = dataset(\"heart\", [1,5,6,8])\n",
    "delinquency_dataset = dataset(\"delinquency\", [9])\n",
    "wine_dataset = dataset(\"wine\", [])\n",
    "paysim_dataset = dataset(\"paysim\", [])\n",
    "\n",
    "# --- Finance Datasets --- \n",
    "\n",
    "dataset_dict = {\n",
    "\t'admissions': admissions_dataset,\n",
    "\t'diabetes': diabetes_dataset,\n",
    "\t'fico': fico_dataset,\n",
    "\t'heart': heart_dataset,\n",
    "\t'delinquency': delinquency_dataset,\n",
    "\t'wine':wine_dataset,\n",
    "\t'paysim':paysim_dataset\n",
    "}\n",
    "\n",
    "\n",
    "## init_data start\n",
    "global data_name, lock, folder_path, data_path, preproc_path, projection_changes_path,reduced_data_path, projection_anchs_path, no_bins, df, model_path, density_fineness, bins_used\n",
    "global categorical_cols, monotonicity_arr, feature_selector_input, feature_names, all_data, data, metadata, target, no_samples, no_features, svm_model, bins_centred, X_pos_array, init_vals\n",
    "global col_ranges, all_den, all_median, all_mean, high_den, high_median, high_mean, low_den, low_median, low_mean, dict_array, dict_array_orig, percentage_filter_input\n",
    "\n",
    "\n",
    "\n",
    "# --- Data initialization ---\n",
    "data_name, lock, folder_path, data_path, preproc_path, projection_changes_path, reduced_data_path, projection_anchs_path, no_bins, df, model_path, density_fineness, bins_used = np.zeros(13)\n",
    "categorical_cols, monotonicity_arr, feature_selector_input, feature_names, all_data, data, metadata, target, no_samples, no_features, svm_model, bins_centred, X_pos_array, init_vals = np.zeros(14)\n",
    "col_ranges, all_den, all_median, all_mean, high_den, high_median, high_mean, low_den, low_median, low_mean, dict_array, dict_array_orig, percentage_filter_input = np.zeros(13)\n",
    "\n",
    "\n",
    "\n",
    "dataset= dataset_dict['heart']\n",
    "\n",
    "dataset.lock\n",
    "\n",
    "# data_name = dataset.name\n",
    "data_name= \"data\"\n",
    "lock = dataset.lock\n",
    "\n",
    "# --- Path Parameters --- \n",
    "folder_path = \"WebApplication/static/data/\" + data_name + '/'\n",
    "data_path = folder_path + data_name + \".csv\"\n",
    "preproc_path = folder_path + data_name + \"_preproc.csv\"\n",
    "projection_changes_path = folder_path + data_name + \"_changes_proj.csv\"\n",
    "projection_anchs_path = folder_path + data_name + \"_anchs_proj.csv\"\n",
    "reduced_data_path = folder_path + data_name + \"_raw_proj\"\n",
    "\n",
    "# print(reduced_data_path)\n",
    "\n",
    "no_bins = 10\n",
    "bins_used = 20\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "X, y = shap.datasets.iris()\n",
    "X[\"target\"]= y\n",
    "df = X\n",
    "\n",
    "\n",
    "model_path = \"TBD\"   # Manual? \n",
    "\n",
    "# --- Advanced Parameters\n",
    "density_fineness = 100\n",
    "categorical_cols = []  # Categorical columns can be customized # Whether there is order\n",
    "# monotonicity_arr = []  # Local test of monotonicity\n",
    "\n",
    "feature_names = np.array(df.columns)[:-1]\n",
    "all_data = np.array(df.values)\n",
    "\n",
    "# --- Split data and target values ---\n",
    "data = all_data[:,:-1]\n",
    "# data = np.array(data, dtype=float)\n",
    "target = all_data[:,-1]\n",
    "\n",
    "# --- Filter data by class ---\n",
    "high_data = all_data[all_data[:,-1] == 1][:,:-1]\n",
    "low_data =  all_data[all_data[:,-1] == 0][:,:-1]\n",
    "\n",
    "no_samples, no_features = data.shape\n",
    "\n",
    "\n",
    "#build a model here. try randomforest first. \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "X, y = shap.datasets.iris()\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "\n",
    "svm_model= external_models()\n",
    "svm_model.set_model(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bins_centred, X_pos_array, init_vals, col_ranges = divide_data_bins(data,no_bins)  # Note: Does not account for categorical features\n",
    "all_den, all_median, all_mean = all_kernel_densities(data,feature_names,density_fineness) # Pre-load density distributions\n",
    "high_den, high_median, high_mean = all_kernel_densities(high_data,feature_names,density_fineness)\n",
    "low_den, low_median, low_mean = all_kernel_densities(low_data,feature_names,density_fineness)\n",
    "\n",
    "monotonicity_arr = mono_finder(svm_model, data, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bins_centred, X_pos_array, init_vals, col_ranges = divide_data_bins(data,no_bins)  # Note: Does not account for categorical features\n",
    "all_den, all_median, all_mean = all_kernel_densities(data,feature_names,density_fineness) # Pre-load density distributions\n",
    "high_den, high_median, high_mean = all_kernel_densities(high_data,feature_names,density_fineness)\n",
    "low_den, low_median, low_mean = all_kernel_densities(low_data,feature_names,density_fineness)\n",
    "\n",
    "monotonicity_arr = mono_finder(svm_model, data, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_centred, X_pos_array, init_vals, col_ranges = divide_data_bins(data,no_bins)  # Note: Does not account for categorical features\n",
    "all_den, all_median, all_mean = all_kernel_densities(data,feature_names,density_fineness) # Pre-load density distributions\n",
    "high_den, high_median, high_mean = all_kernel_densities(high_data,feature_names,density_fineness)\n",
    "low_den, low_median, low_mean = all_kernel_densities(low_data,feature_names,density_fineness)\n",
    "\n",
    "monotonicity_arr = mono_finder(svm_model, data, col_ranges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FEATURE SELECTOR ====\n",
    "# init_vals = [0,10]\n",
    "samples4test = []\n",
    "feature_selector_input = []\n",
    "for i in range(no_features):\n",
    "    feature_selector_input.append(prep_feature_selector(data, i, feature_names, col_ranges, no_bins, samples4test))# 0 indexed\n",
    "# If no init vals known then leave blank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Perform Preprocessing if new data --- \n",
    "if not path.exists(preproc_path): \n",
    "    create_summary_file(data, target, svm_model, bins_centred, X_pos_array, init_vals, no_bins, monotonicity_arr, preproc_path, col_ranges, lock)\n",
    "elif reset:\n",
    "        os.remove(preproc_path)\n",
    "        create_summary_file(data, target, svm_model, bins_centred, X_pos_array, init_vals, no_bins, monotonicity_arr, preproc_path, col_ranges, lock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Projection Files ---\n",
    "if ((not path.exists(projection_changes_path[:-4]+\"_PCA.csv\")) or (not path.exists(projection_anchs_path[:-4]+\"_PCA.csv\"))):\n",
    "    generate_projection_files(preproc_path, data, target, projection_changes_path, projection_anchs_path) \n",
    "elif reset:\n",
    "        os.remove(projection_changes_path[:-4]+\"_PCA.csv\")\n",
    "        os.remove(projection_anchs_path[:-4]+\"_PCA.csv\")\n",
    "        os.remove(projection_changes_path[:-4]+\"_TSNE.csv\")\n",
    "        os.remove(projection_anchs_path[:-4]+\"_TSNE.csv\")\n",
    "        generate_projection_files(preproc_path, data, target, projection_changes_path, projection_anchs_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dimensionality reduction --- \n",
    "if (not path.exists(reduced_data_path+\"_TSNE.csv\")) or (not path.exists(reduced_data_path+\"_PCA.csv\")): \n",
    "    reduce_raw_data(data, reduced_data_path, \"PCA\")\n",
    "    reduce_raw_data(data, reduced_data_path, \"TSNE\")\n",
    "elif reset:\n",
    "    os.remove(reduced_data_path+\"_TSNE.csv\")\n",
    "    os.remove(reduced_data_path+\"_PCA.csv\")\n",
    "    reduce_raw_data(data, reduced_data_path, \"PCA\")\n",
    "    reduce_raw_data(data, reduced_data_path, \"TSNE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Metadata ---\n",
    "metadata = \tpd.read_csv(preproc_path, index_col=False).values\n",
    "\n",
    "# --- Percentage Filter ---\n",
    "samples_selected = [x for x in range(100)]\n",
    "\n",
    "percentage_filter_input = prep_percentage_filter(metadata, bins_used, samples_selected)\n",
    "\n",
    "conf_matrix_input = prep_confusion_matrix(metadata, samples_selected)\n",
    "\n",
    "one_compset = prep_complete_data(metadata, data, feature_names, samples_selected ,col_ranges, bins_centred, X_pos_array, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_params = {\n",
    "    'data_name': data_name,\n",
    "    'lock': lock,\n",
    "    'folder_path': folder_path,\n",
    "    'data_path': data_path,\n",
    "    'preproc_path': preproc_path,\n",
    "    'projection_changes_path': projection_changes_path,\n",
    "    'projection_anchs_path': projection_anchs_path,\n",
    "    'no_bins': no_bins,\n",
    "    'df': df,\n",
    "    'model_path': model_path,\n",
    "    'density_fineness': density_fineness,\n",
    "    'categorical_cols': categorical_cols,\n",
    "    'monotonicity_arr': monotonicity_arr,\n",
    "    'feature_selector_input': feature_selector_input,\n",
    "    'percentage_filter_input': percentage_filter_input,\n",
    "    'feature_names': feature_names,\n",
    "    'all_data': all_data,\n",
    "    'data': data,\n",
    "    'metadata':metadata,\n",
    "    'target': target,\n",
    "    'no_samples': no_samples,\n",
    "    'no_features': no_features,\n",
    "    'svm_model': svm_model,\n",
    "    'bins_centred': bins_centred,\n",
    "    'X_pos_array': X_pos_array,\n",
    "    'init_vals': init_vals,\n",
    "    'col_ranges': col_ranges,\n",
    "    'all_den': all_den,\n",
    "    'all_median': all_median,\n",
    "    'all_mean': all_mean,\n",
    "    'dict_array': dict_array,\n",
    "    'dict_array_orig': dict_array_orig,\n",
    "    'reduced_data_path':reduced_data_path,\n",
    "    'bins_used':bins_used\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter Dictionary ---\n",
    "PD = all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from WebApplication.run import *\n",
    "data_in(PD)\n",
    "run_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
